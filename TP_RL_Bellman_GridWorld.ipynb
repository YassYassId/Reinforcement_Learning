{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83069dd8-8193-460c-9319-b2fed04884b4",
   "metadata": {},
   "source": [
    "# 📍 GridWorld - Value Iteration Algorithm\n",
    "\n",
    "This notebook implements **Value Iteration** to find the optimal policy for a **4x4 GridWorld environment**.  \n",
    "The agent starts at (0,0) and must reach the goal (🏁) at (3,3), avoiding obstacles (█) and penalty zones (🔥).\n",
    "\n",
    "## 🏆 Rules:\n",
    "- The agent can move **Up (↑), Down (↓), Left (←), or Right (→)**.\n",
    "- Each step has a **default reward of -1** (to encourage a shorter path).\n",
    "- The **goal square gives +10** and ends the episode.\n",
    "- **Obstacles (█) are impassable**.\n",
    "- **Penalty squares (🔥) give -3** if stepped on.\n",
    "- The agent uses **Value Iteration** to compute the best path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9a61b-91df-48b3-a9b6-96b2589fa4ea",
   "metadata": {},
   "source": [
    "# 🔢 Understanding Value Iteration\n",
    "\n",
    "Value Iteration is a **Dynamic Programming algorithm** that updates the value of each state using:\n",
    "\n",
    "V(s) = max_a( R(s,a) + gamma * V(s'))\n",
    "\n",
    "Where:\n",
    "- \\( V(s) \\) = Value of state \\( s \\)\n",
    "- \\( R(s, a) \\) = Reward for taking action \\( a \\) from state \\( s \\)\n",
    "- \\( \\gamma \\) = Discount factor (importance of future rewards)\n",
    "- \\( s' \\) = New state after taking action \\( a \\)\n",
    "\n",
    "### 📌 Steps:\n",
    "1. **Initialize** all state values to zero.\n",
    "2. **Iterate until convergence**:\n",
    "   - Update each state's value based on the best possible future reward.\n",
    "3. **Extract the best policy**:\n",
    "   - For each state, pick the action that leads to the highest value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d268d7d5-fe2a-4558-bd9d-8306e27104a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Paramètres de l'environnement\n",
    "GRID_SIZE = 4\n",
    "GAMMA = 0.9  # Facteur de réduction\n",
    "THETA = 1e-4  # Seuil de convergence\n",
    "\n",
    "# Récompenses et obstacles\n",
    "REWARDS = np.full((GRID_SIZE, GRID_SIZE), -1)  # Pénalité de déplacement\n",
    "REWARDS[3, 3] = 10  # Récompense pour la case objectif\n",
    "REWARDS[2, 2] = -3  # Case de pénalité ajoutée\n",
    "OBSTACLES = [(1, 1)]  # Cases infranchissables\n",
    "\n",
    "# Position initiale de l'agent\n",
    "START_POS = (0, 0)\n",
    "\n",
    "# Directions de mouvement\n",
    "ACTIONS = {\n",
    "    '↑': (-1, 0),\n",
    "    '↓': (1, 0),\n",
    "    '←': (0, -1),\n",
    "    '→': (0, 1)\n",
    "}\n",
    "\n",
    "# Initialisation de la valeur des états\n",
    "V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "\n",
    "def is_valid(state):\n",
    "    \"\"\"Vérifie si un état est valide (dans la grille et pas un obstacle).\"\"\"\n",
    "    x, y = state\n",
    "    return 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE and (x, y) not in OBSTACLES\n",
    "\n",
    "def value_iteration():\n",
    "    \"\"\"Itère sur les valeurs des états pour trouver les meilleures actions.\"\"\"\n",
    "    global V\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if (x, y) == (3, 3) or (x, y) in OBSTACLES:\n",
    "                    continue  # Ne pas mettre à jour l'objectif ni les obstacles\n",
    "                \n",
    "                values = []\n",
    "                for action, (dx, dy) in ACTIONS.items():\n",
    "                    new_x, new_y = x + dx, y + dy\n",
    "                    if is_valid((new_x, new_y)):\n",
    "                        values.append(REWARDS[x, y] + GAMMA * V[new_x, new_y])  # Utiliser la case actuelle pour la récompense\n",
    "                    else:\n",
    "                        values.append(REWARDS[x, y] + GAMMA * V[x, y])  # Reste sur place si invalide\n",
    "                \n",
    "                new_V[x, y] = max(values)  # Choisir la meilleure action\n",
    "                delta = max(delta, abs(new_V[x, y] - V[x, y]))\n",
    "        \n",
    "        V = new_V\n",
    "        if delta < THETA:\n",
    "            break\n",
    "\n",
    "def get_policy():\n",
    "    \"\"\"Retourne une politique optimisée sous forme de flèches.\"\"\"\n",
    "    policy = np.full((GRID_SIZE, GRID_SIZE), ' ', dtype=str)\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            if (x, y) == (3, 3):\n",
    "                policy[x, y] = '🏁'  # Arrivée\n",
    "                continue\n",
    "            if (x, y) in OBSTACLES:\n",
    "                policy[x, y] = '█'  # Mur\n",
    "                continue\n",
    "            if (x, y) == (2, 2):\n",
    "                policy[x, y] = '🔥'  # Case de pénalité\n",
    "                continue\n",
    "            \n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for action, (dx, dy) in ACTIONS.items():\n",
    "                new_x, new_y = x + dx, y + dy\n",
    "                if is_valid((new_x, new_y)) and V[new_x, new_y] > best_value:\n",
    "                    best_value = V[new_x, new_y]\n",
    "                    best_action = action\n",
    "            \n",
    "            policy[x, y] = best_action if best_action else ' '\n",
    "    \n",
    "    # Ajout de l'agent 🚀 à la case de départ\n",
    "    policy[START_POS] = '🚀'\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# Exécuter l'algorithme et afficher la politique optimale\n",
    "value_iteration()\n",
    "policy = get_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23344297-f5a3-42a2-8005-15123793225d",
   "metadata": {},
   "source": [
    "# 📊 Results - Optimal Policy\n",
    "\n",
    "- The agent (🚀) starts at (0,0).\n",
    "- The goal (🏁) is at (3,3).\n",
    "- Walls (█) block movement.\n",
    "- A penalty zone (🔥) at (2,2) should be avoided.\n",
    "\n",
    "## ✅ Expected Output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afd86bbf-05f7-467b-89a3-b4ad9f3ba2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 → ↓ ↓\n",
      "↓ █ → ↓\n",
      "↓ ↓ 🔥 ↓\n",
      "→ → → 🏁\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([\" \".join(row) for row in policy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bb02f-ad77-40f1-adc9-f97b0e6d13e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "ce6edd0c-818d-480c-b81b-f9097f95e99d"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
